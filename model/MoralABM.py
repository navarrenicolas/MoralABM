import numpy as np
from scipy.stats import beta, multinomial
import scipy.special as sp

# use the smoothing parameter to influence reliability of the agent
def softmax(x,b=1):
    x=np.array(x)
    return np.exp(b*x)/np.exp(b*x).sum()

class MoralABM():

    def __init__(self,n_agents=10,n_steps=50,beta_prior=False,priors=[],normalize=False):
        ## Define agents' moral distribution parameters
        if beta_prior:
            self.n_params = 10 + 1 # 10 moral representations (betas) + 1 reliability
        else:
            self.n_params = 5 + 1 # 5 moral representations (dirichlet) + 1 reliability
        
        self.normalize = normalize
        
        self.n_agents = n_agents
        self.n_steps = n_steps
        
        # Moral foundations (technically not needed but used for reference)
        self.foundations = ['care', 'fairness', 'ingroup', 'authority', 'purity']
        
        # all agents' beliefs about all other agents' moral foundations at all points in time
        # (n_agents X n_agents X n_params X n_steps)
        self.M_agents = np.zeros((self.n_agents,self.n_agents,self.n_params,self.n_steps)) 
        
        # signals that will be generated by agents
        self.signals = -np.ones((self.n_agents,self.n_steps))

        # participant IDs
        self.agent_ids = np.zeros(n_agents)

        # generated graphs
        self.mf_graph = np.zeros((self.n_steps,self.n_agents,self.n_agents))
        self.belief_graph = np.zeros((self.n_steps,self.n_agents,self.n_agents))

        if not priors:
            # Set moral distribution priors
            # We may want to pass these as an unput in the future
            conservative_priors = [2,3,3,3,4] # np.random.rand(self.n_params-1)+1
            liberal_priors = [4,5,3,1,3] # np.random.rand(self.n_params-1)+1
            self.political_priors = [conservative_priors,liberal_priors]
            # Set initial conditions (distribution and beta parameters)
            for agent_i in range(n_agents):
                # Randomly assign agents to conservative or liberal groups


                # Assume all other agents are random
                self.M_agents[agent_i,:,:(self.n_params-1),0] = np.random.rand(n_agents,self.n_params-1)*6+1
                # # Assume all other agents are the same
                # for agent_a in range(n_agents):
                #     self.M_agents[agent_i,agent_a,:(self.n_params-1),0] = self.M_agents[agent_i,agent_i,:(self.n_params-1),0]
                self.M_agents[agent_i,agent_i,:(self.n_params-1),0] = self.political_priors[np.random.choice([0,1],p=[1/2,1/2])]
        else:
            # If moral foundation priors present

            # Set initial conditions (distribution and beta parameters)
            for agent_i in range(n_agents):

                # first half conservative, second half liberal
                prior = priors[int(agent_i >= n_agents//2)]
                self.agent_ids[agent_i] = int(np.random.choice(prior.index))
                
                
                # Assume all other agents are random
                self.M_agents[agent_i,:,:(self.n_params-1),0] = np.random.rand(n_agents,self.n_params-1)*6+1
                # # Assume all other agents are the same
                # for agent_a in range(n_agents):
                #     self.M_agents[agent_i,agent_a,:(self.n_params-1),0] = self.M_agents[agent_i,agent_i,:(self.n_params-1),0]
                self.M_agents[agent_i,agent_i,:(self.n_params-1),0] =  prior[self.agent_ids[agent_i]]
                    
        
        # set beta (decision) parameters
        self.M_agents[:,:,self.n_params-1,:] = np.ones((self.n_agents,self.n_agents,self.n_steps))

        # Run the simulations
        self._run()

        
    # Sample moral values from base distribution
    def sample_moral_values(self,agent_i,agent_a,step):
        M = self.M_agents[agent_i,agent_a,:(self.n_params-1),step]
        # Hack for beta distribution 
        if self.n_params > 6:
            return [beta.rvs(M[i],M[i+1],size=1)[0] for i in 2*np.arange(5)]
        else:
            return np.random.dirichlet(M, 1)[0]
    
    # generate a moral signal
    def generate_signal(self,agent_i,step):
        morals = self.sample_moral_values(agent_i,agent_i,step)
        # signal = np.random.choice(np.arange(5), p=morals)
        morals_normed = softmax(morals,
                                b=self.M_agents[agent_i,agent_i,self.n_params-1,step]
                               )
        signal = np.random.choice(np.arange(5), p=morals_normed)
        return signal
    
    # compute likelihood of a signal
    def signal_likelihood(self, signal, agent_i, agent_a, step):
        M_i_a = self.sample_moral_values(agent_i,agent_a,step)
        beta = self.M_agents[agent_i,agent_a,self.n_params-1,step]
        # return the probability of the given signal from normalized morals
        return softmax(M_i_a, beta = beta)[int(signal)]

    
    # Can use analytic formulas for the KL divergence of dirichlet and beta distributions
    # https://statproofbook.github.io/P/dir-kl.html
    # NB: Beta version would involve summing up each individual fountation's beta KLdiv
    def KL_divergence(self,agent_p,agent_q,step,belief=True):
        # Belief of agent_q about agent_p's values
        rep_p = self.M_agents[(agent_q if belief else agent_p),agent_p,:(self.n_params-1),step] 
        rep_q = self.M_agents[agent_q,agent_q,:(self.n_params-1),step] # agent_q's actual values

        if self.n_params > 6 :
            KL_div = sum([self._dirichlet_KL(rep_p[m:m+2],rep_q[m:m+2]) for m in 2*np.arange(5)])
        else:
            KL_div = self._dirichlet_KL(rep_p,rep_q)
        
        if KL_div < 0:
            print('Oops! KL divergence is negative.', f'P: {agent_p}, Q: {agent_q}, step: {step}')
        return KL_div

    def _dirichlet_KL(self,p,q):

            p_sum = sum(p)
            q_sum = sum(q)

            term1 = sp.gammaln(p_sum) - sp.gammaln(q_sum)
            term2 = sp.gammaln(q).sum() - sp.gammaln(p).sum()
            term3 = np.sum([(alpha_p-alpha_q) * (sp.psi(alpha_p)-sp.psi(p_sum)) for alpha_p,alpha_q in zip(p,q)])

            return term1 + term2 + term3

    def update_morality(self,step):

        if self.normalize:
            for agent_i in range(self.n_agents):
                for agent_a in range(self.n_agents):
                    if self.n_params>6:
                        for i in 2*np.arange(5):
                            unnormed_vals = self.M_agents[agent_i,agent_a,i:+2,step]
                            self.M_agents[agent_i,agent_a,i:+2,step] = unnormed_vals/sum(unnormed_vals)
                    else:
                        unnormed_vals = self.M_agents[agent_i,agent_a,:(self.n_params-1),step]
                        self.M_agents[agent_i,agent_a,:(self.n_params-1),step] = unnormed_vals/sum(unnormed_vals)
            
        self.M_agents[:,:,:,step+1] = self.M_agents[:,:,:,step] # copy last state
        for agent_i in range(self.n_agents):
            for agent_a in range(self.n_agents):
                signal_a = int(self.signals[agent_a,step])
                if self.n_params >6:
                    self.M_agents[agent_i,agent_a,:(self.n_params-1),step+1][2*signal_a] += 1 # Beta-Multinomial update
                    # Increase the b parameter of the other moral foundations by 1/4 each (evidence they chose against one of these foundations)
                    b_locs = np.delete((2*np.arange(5))+1,signal_a)
                    self.M_agents[agent_i,agent_a,:(self.n_params-1),step+1][b_locs] += 1/4 # Beta-Multinomial update
                else:
                    self.M_agents[agent_i,agent_a,:(self.n_params-1),step+1][signal_a] += 1 # Dirichlet-Multinomial update
                if agent_i == agent_a:
                    continue

                # Compute and save the KL divergencces of the previous state
                self.belief_graph[step,agent_i,agent_a] = self.KL_divergence(agent_a,agent_i,step,belief=True)
                self.mf_graph[step,agent_i,agent_a] = self.KL_divergence(agent_a,agent_i,step,belief=False)
                
                # Weighted update
                update_weight = np.exp(-self.belief_graph[step,agent_i,agent_a])/(self.n_agents-1) # Dirichlet-Multinomial update 
                

                if self.n_params >6:
                    self.M_agents[agent_i,agent_i,:(self.n_params-1),step+1][2*signal_a] += update_weight
                    b_locs = np.delete((2*np.arange(5))+1,signal_a)
                    self.M_agents[agent_i,agent_i,:(self.n_params-1),step+1][b_locs] += update_weight/4
                else:
                    self.M_agents[agent_i,agent_i,:(self.n_params-1),step+1][signal_a] += update_weight

    # run from initial conditions
    def _run(self,):
        for step in range(self.n_steps-1):
            # generate the signals
            self.signals[:,step] = [self.generate_signal(agent_i,step) for agent_i in range(self.n_agents)]
            self.update_morality(step,)
    
    def get_adjacency_matrix(self,step,belief=False):
        network = np.zeros((self.n_agents,self.n_agents))
        for agent_i in range(self.n_agents):
            for agent_a in range(self.n_agents):
                network[agent_i,agent_a] = self.KL_divergence(agent_a,agent_i,step,belief=belief)
        return network

    
        
        
            
