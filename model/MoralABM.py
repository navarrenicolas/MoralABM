import numpy as np
from scipy.stats import beta, multinomial
import scipy.special as sp

# use the smoothing parameter to influence reliability of the agent
def softmax(x,beta=1):
    return np.exp(beta*x)/np.exp(beta*x).sum()

class MoralABM():

    def __init__(self,n_agents=10,n_steps=50,representation='dirichlet',priors=[]):
        ## Define agents' moral distribution parameters
        if representation == 'dirichlet':
            self.n_params = 5 + 1 # 5 moral representations (dirichlet) + 1 reliability
        else:
            self.n_params = 10 + 1 # 10 moral representations (betas) + 1 reliability
        
        self.n_agents = n_agents
        self.n_steps = n_steps
        
        # Moral foundations (technically not needed but used for reference)
        self.foundations = ['care', 'fairness', 'ingroup', 'authority', 'purity']
        
        # all agents' beliefs about all other agents' moral foundations at all points in time
        # (n_agents X n_agents X n_params X n_steps)
        self.M_agents = np.zeros((self.n_agents,self.n_agents,self.n_params,self.n_steps)) 
        
        # signals that will be generated by agents
        self.signals = -np.ones((self.n_agents,self.n_steps))

        # participant IDs
        self.agent_ids = np.zeros(n_agents)

        # If moral foundation priors present
        if not priors:
            # Set moral distribution priors
            # We may want to pass these as an unput in the future
            conservative_priors = [2,3,3,3,4] # np.random.rand(self.n_params-1)+1
            liberal_priors = [4,5,3,1,3] # np.random.rand(self.n_params-1)+1
            self.political_priors = [conservative_priors,liberal_priors]
            # Set initial conditions (distribution and beta parameters)
            for agent_i in range(n_agents):
                # Randomly assign agents to conservative or liberal groups
                self.M_agents[agent_i,agent_i,:(self.n_params-1),0] = self.political_priors[np.random.choice([0,1],
                                                                                                        p=[1/2,1/2]
                                                                                                       )]
                # Assume all other agents are the same
                for agent_a in range(n_agents):
                    self.M_agents[agent_i,agent_a,:(self.n_params-1),0] = self.M_agents[agent_i,agent_i,:(self.n_params-1),0]
        else:
            conservative_priors = priors[0]
            liberal_priors = priors[1]
            # Set initial conditions (distribution and beta parameters)
            for agent_i in range(n_agents):

                # first half conservative, second half liberal
                prior = priors[int(agent_i >= n_agents//2)]
                self.agent_ids[agent_i] = int(np.random.choice(prior.index))
                self.M_agents[agent_i,agent_i,:(self.n_params-1),0] =  prior[self.agent_ids[agent_i]]
                
                # Assume all other agents are the same
                for agent_a in range(n_agents):
                    self.M_agents[agent_i,agent_a,:(self.n_params-1),0] = self.M_agents[agent_i,agent_i,:(self.n_params-1),0]
        
        # set beta (decision) parameters
        self.M_agents[:,:,self.n_params-1,:] = np.ones((self.n_agents,self.n_agents,self.n_steps))

        # Run the simulations
        self._run()

        
    # Saple moral values from base distribution
    def sample_moral_values(self,agent_i,agent_a,step):
        M = self.M_agents[agent_i,agent_a,:(self.n_params-1),step]
        # Hack for dirichlet version
        if self.n_params > 6:
            return [beta.rvs(M[i],M[i+1],size=1)[0] for i in 2*np.arange(5)]
        else:
            return np.random.dirichlet(M, 1)[0]
    
    # generate a moral signal
    def generate_signal(self,agent_i,step):
        morals = self.sample_moral_values(agent_i,agent_i,step)
        # signal = np.random.choice(np.arange(5), p=morals)
        morals_normed = softmax(morals,
                                beta=self.M_agents[agent_i,agent_i,self.n_params-1,step]
                               )
        signal = np.random.choice(np.arange(5), p=morals_normed)
        return signal
    
    # compute likelihood of a signal
    def signal_likelihood(self, signal, agent_i, agent_a, step):
        M_i_a = self.sample_moral_values(agent_i,agent_a,step)
        beta = self.M_agents[agent_i,agent_a,self.n_params-1,step]
        # return the probability of the given signal from normalized morals
        return softmax(M_i_a, beta = beta)[int(signal)]

    
    # Can use analytic formulas for the KL divergence of dirichlet and beta distributions
    # https://statproofbook.github.io/P/dir-kl.html
    # NB: Beta version would involve summing up each individual fountation's beta KLdiv
    def KL_divergence(self,agent_p,agent_q,step,belief=True):
        # Belief of agent_q about agent_p's values
        rep_p = self.M_agents[(agent_q if belief else agent_p),agent_p,:(self.n_params-1),step] 
        rep_q = self.M_agents[agent_q,agent_q,:(self.n_params-1),step] # agent_q's actual values
    
        p_sum = rep_p.sum()
        q_sum = rep_q.sum()
        
        if self.n_params > 6 :
            # Beta KL div
            return
        # KL_div = np.log(gamma(p_sum)/gamma(q_sum)) + np.log(gamma(rep_q)/gamma(rep_p)).sum() + np.sum([(alpha_p-alpha_q) * (digamma(alpha_p)-digamma(p_sum)) for alpha_p,alpha_q in zip(rep_p,rep_q)])
        
        term1 = sp.gammaln(p_sum) - sp.gammaln(q_sum)
        term2 = sp.gammaln(rep_q).sum() - sp.gammaln(rep_p).sum()
        term3 = np.sum([(alpha_p-alpha_q) * (sp.psi(alpha_p)-sp.psi(p_sum)) for alpha_p,alpha_q in zip(rep_p,rep_q)])

        KL_div = term1 + term2 + term3
        
        if KL_div < 0:
            print('Oops! KL divergence is negative.', f'P: {agent_p}, Q: {agent_q}, step: {step}')
        return KL_div


    def update_morality(self,step):
        self.M_agents[:,:,:,step+1] = self.M_agents[:,:,:,step] # copy last state
        for agent_i in range(self.n_agents):
            for agent_a in range(self.n_agents):
                signal_a = int(self.signals[agent_a,step])
                self.M_agents[agent_i,agent_a,:(self.n_params-1),step+1][signal_a] += 1 # Dirichlet-Multinomial update
                if agent_i == agent_a:
                    continue
                # Weighted update
                self.M_agents[agent_i,agent_i,:(self.n_params-1),step+1][signal_a] += np.exp(
                    -self.KL_divergence(agent_a,agent_i,step)
                )/(self.n_agents-1) # Dirichlet-Multinomial update 

    # run from initial conditions
    def _run(self):
        for step in range(self.n_steps-1):
            # generate the signals
            self.signals[:,step] = [self.generate_signal(agent_i,step) for agent_i in range(self.n_agents)]
            self.update_morality(step)
    
    def get_adjacency_matrix(self,step,belief=False):
        network = np.zeros((self.n_agents,self.n_agents))
        for agent_i in range(self.n_agents):
            for agent_a in range(self.n_agents):
                network[agent_i,agent_a] = self.KL_divergence(agent_a,agent_i,step,belief=belief)
        return network

    
        
        
            